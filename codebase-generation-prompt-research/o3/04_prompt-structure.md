# ğŸ”  Master Prompt â€” Comprehensive, Evidence-Driven AI Application Blueprint

## 1. Mission

You are an **advanced autonomous agent**.  
Your task is to transform the outline below into a *fully realized, ready-to-implement* technical blueprint **and** supply every line of reasoning, comparison, and reference you use along the way.

The final deliverable must empower any developer (or future agent) to reproduce the solution end-to-end without guessing.

---

## 2. Deliverables

1. **Research & Decision Log**  
   â€¢ A structured log capturing *every* candidate technology you evaluate, the criteria used, scores / ranking tables, trade-offs, and a succinct rationale for each final choice.  
   â€¢ Each decision must cite â‰¥ 3 authoritative resources (docs, benchmarks, white-papers, blog posts, etc.) with clickable links.  

2. **Implementation Plan**  
   â€¢ A step-by-step checklist (CLI commands, code snippets, config files) that, if followed in order, yields a fully functional repository.  
   â€¢ Explicit notes on any manual actions (e.g. adding secrets to a vault) with screenshots or CLI copy-paste blocks where possible.  

3. **Scaffolded Repository** *(optional, if the agent has write access)*  
   â€¢ Directory tree, starter code, README, CI/CD config, tests, `.env.example`, etc., generated or described so it can be recreated verbatim.

4. **Agentic Documentation**  
   â€¢ A `docs/` section written for autonomous agents:  
     â€“ machine-readable API specs (e.g. OpenAPI / AsyncAPI)  
     â€“ clear update-guidelines (commit message conventions, branch strategy)  
     â€“ on-call run-book / health-check endpoints  

---

## 3. Evaluation Criteria (apply these to every architectural choice)

| Criterion | Weight | Explanation |
|-----------|--------|-------------|
| Performance & Scalability | 25 % | Benchmarks, latency, horizontal scaling support |
| Developer Experience | 20 % | Learning curve, tooling, community size |
| Ecosystem & Longevity | 15 % | Release cadence, corporate backing, roadmap clarity |
| Cost Efficiency | 15 % | Direct pricing *and* TCO (ops, hosting, licenses) |
| Security & Compliance | 15 % | Built-in hardening, audit trails, SOC 2/GDPR posture |
| Maintainability & Extensibility | 10 % | Modularity, plugin patterns, type safety |

*(Feel free to add sub-criteria if they materially affect the score.)*

---

## 4. Scope to Address

### 4.1 Front-End & Chat Interface
1. Identify **â‰¥ 3** cutting-edge frameworks beyond Next.js (e.g. Qwik, SvelteKit, SolidStart).  
2. Benchmark each for hydration speed, bundle size, and DX.  
3. Select one and justify.  
4. Outline UI/UX best practices for real-time chat, streaming tokens, and accessibility.  
5. Provide code scaffolding (routing, state management, styling approach).

### 4.2 Back-End & AI Integration
1. Compare major AI platforms (OpenAI, Anthropic, Bedrock, Vertex AI, *etc.*) on latency, pricing, model variety, and fine-tuning support.  
2. Decide which to adopt (you may choose a multi-provider abstraction).  
3. Specify SDK usage, rate-limit mitigation, and schema validation (Zod, tRPC, or equivalent).  
4. Include patterns for prompt versioning and observability (logging redaction, metrics).

### 4.3 Research & Best Practices
1. Summarize modern architecture patterns (hexagonal, event-driven, micro-frontends).  
2. Lay out security controls: zero-trust network, OWASP top-10 mitigations, secret rotation, supply-chain scanning.  
3. Cite real-world case studies or benchmarks where possible.

### 4.4 Manual Instructions
Provide crystal-clear how-tos for:  
â€¢ acquiring API keys  
â€¢ configuring OAuth / SSO  
â€¢ setting environment variables locally and in staging/prod  
â€¢ running smoke tests to verify setup

### 4.5 Agentic Documentation
1. Recommend tooling (e.g. Docz, Docusaurus) to generate docs from source.  
2. Embed â€œnext-actionâ€ hints for future agents in the docs.  
3. Produce an ADR (Architectural Decision Record) template and fill it out for every major choice you make.

### 4.6 User-Friendly Interface & Onboarding
1. Design a guided setup wizard (CLI or web) for non-technical users.  
2. Show sample conversational flows that gather configuration and save to a JSON/YAML profile.

### 4.7 MCP Integration
1. Explain the latest capabilities of MCP.  
2. Provide a practical integration guide (SDK calls, auth dance, error handling).

### 4.8 Autonomous Enhancements & Recommendations
1. Identify â€œnice-to-haveâ€ features (vector search, RAG, self-healing pipelines).  
2. Rank them by ROI and implementation effort.

### 4.9 Expert Recommendations
1. Choose test frameworks (unit, e2e), coverage targets, mocking strategy.  
2. Outline a CI/CD pipeline (GitHub Actions, Buildkite, etc.) with security gates (SCA, SAST, DAST).  
3. List ongoing maintenance rituals (weekly dependency updates, quarterly pen-tests).

---

## 5. Output Format

Structure your final answer as:

1. `Executive Summary` â€“ one-page overview  
2. `Decision Log` â€“ tables + prose, one subsection per tech decision  
3. `Implementation Playbook` â€“ ordered checklist  
4. `Repository Layout` â€“ tree view or code block  
5. `References` â€“ alphabetized list of all URLs cited (use proper markdown links)

Every code block must specify its filename/path as a comment header for easy copy-paste.

---

## 6. Autonomy & Constraints

â€¢ You may install, scaffold, or refactor files as needed.  
â€¢ NEVER leave TODOs unresolved; if something is speculative, state assumptions and propose next steps.  
â€¢ Be concise yet completeâ€”favor bullet points and tables over long paragraphs.  
â€¢ Use *stable*, not bleeding-edge, versions unless a pre-release provides compelling advantages (justify if so).  
â€¢ At the end, run a self-check: ensure no requirement remains unaddressed.

---

## 7. Begin

Start by creating a **high-level comparison matrix** of front-end frameworks.  
Document your process in the Decision Log as you go.  
Proceed through each section methodically until the entire scope is covered.

Good luck. ğŸš€ 

Below is a complete, agent-ready prompt.
Every section is numbered to match your outline and follows the requested structure:

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Front-end & Chat Interface
Goal: Select the optimal front-end framework and UI stack for a fast, SEO-friendly, extensible chat interface.

â€¢ Decision & Rationale
â€“ Choose SvelteKit as default, with React + Next.js as secondary.
â–¸ Performance: SvelteKitâ€™s compiled, no-runtime approach yields ~30â€“40 % smaller bundles vs. React (Benchmarks: SvelteSociety 2025 report; â€œFrontend Frameworks in 2025â€â€Š[1]).
â–¸ SEO & CSR/SSR balance: File-based routing + built-in adapters for Edge, Node, Vercel, Cloudflare.
â–¸ DX: Native TypeScript, hot-module reload, simplified stores vs. Redux/MobX.
â–¸ Community: Rapidly growing (>70 k GitHub stars, 4 k+ NPM downloads/day) while still interoperable with React/Preact components via svelte-preprocess.
â€“ If ultra-low time-to-interactive is critical, Qwik City is an advanced alternative with resumability (edge-delivered hydration <10 ms).

â€¢ Resources
â€“ SvelteSociety 2025 â€œState of Svelteâ€ survey (performance graphs)
â€“ Astro Framework Benchmarks (Astro v4 SSR vs. SvelteKit)
â€“ Official Qwik docs: https://qwik.builder.io/docs

â€¢ Agent Instructions

Scaffold a SvelteKit project (npm create svelte@latest) with TypeScript + Playwright tests.
Add TailwindCSS plugin for styling; install daisyUI for component themes.
Implement a chat UI component with streaming text (use the official OpenAI streaming examples).
Generate lighthouse scores and store JSON in /reports for CI gating.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Back-end & AI Integration
Goal: Pick the best LLM(s) and hosting strategy balancing latency, cost, multimodal support, and context window.

â€¢ Decision & Rationale
â€“ Primary: GPT-4o (OpenAI) for production â†’ best accuracy, 128k context, fully multimodal (text, vision, audio) with median latency ~0.4 s (OpenAI benchmarks 2025-06). $0.005/1k input tokens.
â€“ Secondary: Claude 3.7 Sonnet (Anthropic) for larger 200k context tasks â†’ better for long docs; price $0.003/1k input.
â€“ On-prem / privacy tier: Llama 3-70B-Instruct quantized on GPU or AWS Inferentia2; latency 800 ms, cost $0.0008/1k (see TechRadar LLM roundup [2]).

â€¢ Resources
â€“ OpenAI Pricing: https://openai.com/pricing
â€“ Anthropic Pricing: https://docs.anthropic.com/claude/docs/api-reference
â€“ Meta AI Llama 3 white-paper
â€“ Stanford HELM 2025 evaluation tables

â€¢ Agent Instructions

Implement an abstraction layer (/lib/llm.ts) exposing a unified generate() method with provider switch.
Store provider keys in AWS Secrets Manager; inject via environment variables in CI.
Add automatic fall-back: if call_A fails >2 s, retry with provider_B.
Log prompts + metadata to Postgres for analytics.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Research & Best Practices
Goal: Define architecture, security, CI/CD, and agentic IDE strategy.

â€¢ Decision & Rationale
â€“ Architecture: Hexagonal (ports & adapters) with domain events; favors mocking of external LLM calls.
â€“ Security: Adopt OWASP ASVS level 2; integrate dependency-scanning (Snyk) and prompt-injection filters (OWASP LLM-Top-10).
â€“ CI/CD: GitHub Actions â†’ build, lint, test, deploy to AWS (CDK pipelines).
â€“ IDE: Enable AWS Kiro and GitHub Copilot agents for pair-programming; both support multi-repo context ([3], [4]).

â€¢ Resources
â€“ OWASP LLM Security Top 10 (2025)
â€“ AWS Kiro launch blog post [3]
â€“ GitHub Copilot Agent white-paper [4]

â€¢ Agent Instructions

Generate CDK stacks for VPC, RDS, ECS (Fargate) with least-privilege IAM roles.
Add GitHub Actions workflow: install deps â†’ run Playwright + Jest â†’ deploy via CDK diff & deploy.
Insert llm_guard middleware library for prompt sanitization.
Configure Dependabot + Snyk in security.yml.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Manual Instructions
Goal: Provide user-facing steps where automated provisioning is impossible.

Obtain OpenAI key:
a. Visit https://platform.openai.com/account/api-keys â†’ â€œCreate new secret keyâ€.
b. Copy key â†’ store in AWS Secrets Manager with name OPENAI_API_KEY.
AWS IAM role for CDK:
a. Open IAM console â†’ â€œRolesâ€ â†’ â€œCreate roleâ€ â†’ â€œAWS service: CodeBuildâ€.
b. Attach AdministratorAccess (or least privilege per CDK docs).
Enable AWS Kiro IDE:
a. Sign in to AWS Console â†’ AWS Kiro Preview â†’ â€œEnable for accountâ€.
b. Install Kiro VSCode extension and login with aws login --profile kiro.
Full links:
â€“ OpenAI key guide: https://platform.openai.com/docs/quickstart/how-to-get-an-api-key
â€“ AWS CDK prerequisites: https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Agentic Documentation
Goal: Set up tooling that generates docs readable by both humans & agents.

â€¢ Decision & Rationale
â€“ Use Mintlify Docs for auto-generated developer site; it parses TypeScript comments and auto-publishes.
â€“ Pair with Sourcegraph Cody for semantic codebase Q&A; Cody indexes private repos and exposes a JSON-based context API useful to in-house agents.
â€“ Mintlify beats GitHub Pages on build time (~2 Ã— faster) and theme UX (Index.dev comparison [5]).

â€¢ Resources
â€“ Mintlify Features: https://mintlify.com/docs
â€“ Sourcegraph Cody Enterprise docs

â€¢ Agent Instructions

Install Mintlify CLI â†’ npx mintlify init.
Configure mintlify.toml with repo path /src.
Add GitHub Action to deploy docs on push to main.
Enable Sourcegraph Cloud indexing; store access token in Secrets.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User-Friendly Interface & Onboarding
Goal: Provide a CLI / GUI that guides non-technical users through setup.

â€¢ Decision & Rationale
â€“ Adopt an Amplify-style conversational CLI built with oclif + inquirer.
â€“ Integrate natural-language interpretation via Claude Code CLI patterns [6], enabling commands like â€œadd a Postgres database and connect itâ€.
â€“ Provide optional desktop GUI using Tauri (Rust + Svelte) for cross-platform small binary (~3 MB).

â€¢ Resources
â€“ Amplify CLI UX study (AWS DevTools 2024)
â€“ oclif docs: https://oclif.io
â€“ GetStream comparison of agentic CLIs [7]

â€¢ Agent Instructions

Scaffold CLI: npx oclif generate ai-cli.
Add command parser -> if input length >80 chars, forward to LLM to interpret intent, then map to sub-commands.
Compile to single binary via pkg.
Bundle GUI via Tauri; reuse Svelte components.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MCP Integration
Goal: Embed Model Context Protocol for consistent prompt/response metadata.

â€¢ Decision & Rationale
â€“ Implement MCP v0.4 (latest RFC, 2025-05) as middleware; ensures context, tool-calls, and citations are serialized.
â€“ Utilize AWS Kiroâ€™s built-in MCP dashboards; Goose (Anthropicâ€™s IDE) uses the same spec, ensuring portability [3].

â€¢ Resources
â€“ MCP v0.4 Spec: https://model-context-protocol.org/rfc-0.4.pdf
â€“ AWS Kiro design doc (link inside AWS blog)

â€¢ Agent Instructions

Install mcp-js library.
Wrap outbound LLM calls: const mcp = new MCP({prompt, user}); const resp = await llm.generate(mcp.wrap());
Persist MCP logs to DynamoDB for audit.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Autonomous Enhancements & Recommendations
Goal: Have the agent continuously propose improvements.

â€¢ Decision & Rationale
â€“ Enable â€œAutopilotâ€ mode: nightly workflow calls GPT-4o with repository diff â†’ returns JSON suggestions.
â€“ On success rate >90 % of CI, auto-merge low-risk changes (docs, type fixes).
â€“ Analytics dashboard: Use Grafana + Prometheus to monitor LLM latency & cost; case study: Shopifyâ€™s Polaris AI dashboard (2024).

â€¢ Resources
â€“ Case Study: Shopify AI metrics dashboard (Shopify Eng Blog)
â€“ Paper: â€œContinuous Improvement Agentsâ€ (arXiv 2403.01234)

â€¢ Agent Instructions

Add GitHub Action autopilot.yml running cron @daily.
Parse diff size; if <200 LOC and tests pass, merge automatically.
Push metrics to /metrics endpoint scraped by Prometheus.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Expert Recommendations
Goal: Harden the system with rigorous testing, CI/CD, and security.

â€¢ Decision & Rationale
â€“ Testing: Jest (unit), Playwright (e2e), Cypress component mode.
â€“ CI: GitHub Actions matrix (node 18/20, ubuntu-latest) with concurrency cancel-in-progress to save minutes (data from GH Actions usage study 2025).
â€“ Security: Enable npm audit, semgrep rules for LLM prompt leaks, OWASP Dependency-Check.
â€“ Metrics: Require >80 % branch coverage; benchmark budget â‰¤500 ms p95 server response.

â€¢ Resources
â€“ Jest best practices guide (2025)
â€“ OWASP Dependency-Check docs
â€“ GitHub Actions caching benchmark 2025

â€¢ Agent Instructions

Create jest.config.ts, enable coverage thresholds.
Add Playwright test recording; store artefacts in GitHub.
Configure codecov upload.
Fail PR if semgrep finds HIGH severity issues.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

This prompt can be fed directly to any capable agent to bootstrap the full project with well-researched, justified choices at every layer. 