#!/usr/bin/env python3
"""
Main experiment runner for the two-stage adaptive agent implementation experiment.
"""

import os
import json
import subprocess
import shutil
from datetime import datetime
from pathlib import Path
import sys

# Model configurations
MODELS = {
    'o3': {
        'name': 'O3',
        'prompt_file': 'synthesize-research-prompts/o3/02_AGENT_IMPLEMENTATION_PROMPT.md',
        'env_var': 'O3_API_KEY'
    },
    'claude-4-sonnet': {
        'name': 'Claude-4-Sonnet', 
        'prompt_file': 'synthesize-research-prompts/claude-4-sonnet/AGENT_IMPLEMENTATION_PROMPT.md',
        'env_var': 'CLAUDE_API_KEY'
    },
    'claude-4-opus': {
        'name': 'Claude-4-Opus',
        'prompt_file': 'synthesize-research-prompts/claude-4-opus/AGENT_IMPLEMENTATION_PROMPT.md',
        'env_var': 'CLAUDE_API_KEY'
    }
}

class ExperimentRunner:
    """Runs the two-stage adaptive experiment."""
    
    def __init__(self):
        self.experiment_date = datetime.now().strftime('%Y-%m-%d')
        self.experiment_dir = Path(f'/workspace/experiments/{self.experiment_date}')
        self.baseline_dir = self.experiment_dir / 'baseline'
        self.cross_dir = self.experiment_dir / 'cross'
        
    def setup(self):
        """Setup experiment directories."""
        self.baseline_dir.mkdir(parents=True, exist_ok=True)
        self.cross_dir.mkdir(parents=True, exist_ok=True)
        print(f"Experiment directories created at {self.experiment_dir}")
        
    def run_implementation(self, model_key: str, prompt_file: str, output_dir: Path):
        """Run a single model implementation."""
        print(f"\n{'='*60}")
        print(f"Running {MODELS[model_key]['name']} implementation...")
        print(f"Output directory: {output_dir}")
        
        # Create model output directory
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Read the prompt
        prompt_path = Path('/workspace') / prompt_file
        if not prompt_path.exists():
            print(f"ERROR: Prompt file not found: {prompt_path}")
            return False
            
        with open(prompt_path, 'r') as f:
            prompt_content = f.read()
            
        # Simulate implementation generation
        # In a real scenario, this would call the actual model API
        print(f"Generating implementation using {model_key}...")
        
        # For now, create a mock implementation
        self._create_mock_implementation(output_dir, model_key)
        
        print(f"Implementation complete for {model_key}")
        return True
        
    def _create_mock_implementation(self, output_dir: Path, model_key: str):
        """Create a mock implementation for testing."""
        # Create basic project structure
        (output_dir / 'src').mkdir(exist_ok=True)
        (output_dir / 'tests').mkdir(exist_ok=True)
        (output_dir / 'docs').mkdir(exist_ok=True)
        
        # Create main implementation file
        main_content = f"""# Multi-Agent System Implementation
# Generated by {model_key}

class AgentSystem:
    def __init__(self):
        self.name = "{model_key}_implementation"
        
    def execute(self, task):
        return f"Task executed by {{self.name}}"
"""
        
        with open(output_dir / 'src' / 'main.py', 'w') as f:
            f.write(main_content)
            
        # Create test file
        test_content = """import pytest
from src.main import AgentSystem

def test_agent_system():
    agent = AgentSystem()
    result = agent.execute("test task")
    assert "executed" in result
"""
        
        with open(output_dir / 'tests' / 'test_main.py', 'w') as f:
            f.write(test_content)
            
        # Create Dockerfile
        dockerfile_content = """FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install pytest
CMD ["python", "-m", "src.main"]
"""
        
        with open(output_dir / 'Dockerfile', 'w') as f:
            f.write(dockerfile_content)
            
        # Create setup.py
        setup_content = """from setuptools import setup, find_packages

setup(
    name="agent-system",
    version="1.0.0",
    packages=find_packages(),
)
"""
        
        with open(output_dir / 'setup.py', 'w') as f:
            f.write(setup_content)
            
    def run_stage_a(self):
        """Stage A: Baseline runs - each model with its own prompt."""
        print("\n" + "="*80)
        print("STAGE A: BASELINE EXPERIMENTS")
        print("="*80)
        
        results = {}
        
        for model_key, model_config in MODELS.items():
            output_dir = self.baseline_dir / model_key
            success = self.run_implementation(
                model_key,
                model_config['prompt_file'],
                output_dir
            )
            results[model_key] = success
            
        # Save stage A results
        with open(self.experiment_dir / 'stage_a_results.json', 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'results': results
            }, f, indent=2)
            
        return results
        
    def evaluate_baseline(self):
        """Evaluate baseline implementations."""
        print("\n" + "="*80)
        print("EVALUATING BASELINE IMPLEMENTATIONS")
        print("="*80)
        
        # Run evaluation script
        eval_script = Path('/workspace/scripts/eval.py')
        if not eval_script.exists():
            print("ERROR: Evaluation script not found")
            return None
            
        result = subprocess.run([
            sys.executable,
            str(eval_script),
            '--experiment-dir', str(self.experiment_dir),
            '--output', str(self.experiment_dir / 'baseline_evaluation.json')
        ], capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"ERROR: Evaluation failed: {result.stderr}")
            return None
            
        # Read evaluation results
        with open(self.experiment_dir / 'baseline_evaluation.json', 'r') as f:
            eval_results = json.load(f)
            
        winner = eval_results.get('baseline_winner')
        print(f"\nBaseline winner: {winner}")
        
        return winner, eval_results
        
    def run_stage_b(self, winner_model: str):
        """Stage B: Cross runs - winner prompt with other models."""
        print("\n" + "="*80)
        print("STAGE B: CROSS-RUN EXPERIMENTS")
        print(f"Using prompt from winner: {winner_model}")
        print("="*80)
        
        # Get winner's prompt file
        winner_prompt = MODELS[winner_model]['prompt_file']
        
        results = {}
        
        # Run other models with winner's prompt
        for model_key in MODELS:
            if model_key != winner_model:
                output_dir = self.cross_dir / f"{model_key}_with_{winner_model}_prompt"
                success = self.run_implementation(
                    model_key,
                    winner_prompt,
                    output_dir
                )
                results[model_key] = success
                
        # Save stage B results
        with open(self.experiment_dir / 'stage_b_results.json', 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'winner_model': winner_model,
                'winner_prompt': winner_prompt,
                'results': results
            }, f, indent=2)
            
        return results
        
    def final_evaluation(self):
        """Run final evaluation including cross-run results."""
        print("\n" + "="*80)
        print("FINAL EVALUATION")
        print("="*80)
        
        # The eval script will automatically detect and evaluate cross results
        result = subprocess.run([
            sys.executable,
            '/workspace/scripts/eval.py',
            '--experiment-dir', str(self.experiment_dir),
            '--output', str(self.experiment_dir / 'final_evaluation.json')
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            with open(self.experiment_dir / 'final_evaluation.json', 'r') as f:
                final_results = json.load(f)
                
            print(f"\nFinal winner: {final_results.get('final_winner', 'N/A')}")
            return final_results
        else:
            print(f"ERROR: Final evaluation failed: {result.stderr}")
            return None
            
    def synthesize_results(self, baseline_eval, final_eval):
        """Synthesize all results and prepare final artifacts."""
        print("\n" + "="*80)
        print("SYNTHESIZING RESULTS")
        print("="*80)
        
        # Copy winning implementation to final location
        final_winner = final_eval.get('final_winner', baseline_eval.get('baseline_winner'))
        
        if final_winner:
            # Determine source directory
            if final_winner in MODELS:
                source_dir = self.baseline_dir / final_winner
            else:
                # It's from cross-run
                source_dir = self.cross_dir / final_winner
                
            if source_dir.exists():
                dest_dir = Path('/workspace')
                
                # Copy implementation files
                for subdir in ['src', 'infra', 'prompts']:
                    dest_subdir = dest_dir / subdir
                    dest_subdir.mkdir(exist_ok=True)
                    
                    source_subdir = source_dir / subdir
                    if source_subdir.exists():
                        shutil.copytree(source_subdir, dest_subdir, dirs_exist_ok=True)
                        
                # Copy CI/CD configuration
                ci_dir = dest_dir / '.github' / 'workflows'
                ci_dir.mkdir(parents=True, exist_ok=True)
                
                # Create CI configuration (would be copied from winner in real scenario)
                self._create_ci_config(ci_dir)
                
                print(f"Winner implementation copied to workspace root")
                
        return final_winner
        
    def _create_ci_config(self, ci_dir: Path):
        """Create CI/CD configuration."""
        ci_content = """name: CI Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model: [o3, claude-4-sonnet, claude-4-opus]
        
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest radon semgrep
        
    - name: Run tests
      run: pytest tests/
      
    - name: Security scan
      run: semgrep --config=auto .
      
    - name: Upload coverage
      uses: codecov/codecov-action@v3
"""
        
        with open(ci_dir / 'ci.yml', 'w') as f:
            f.write(ci_content)
            
    def run(self):
        """Run the complete experiment."""
        print("Starting Two-Stage Adaptive Experiment")
        print(f"Date: {self.experiment_date}")
        
        # Setup
        self.setup()
        
        # Stage A: Baseline
        stage_a_results = self.run_stage_a()
        
        # Evaluate baseline
        winner, baseline_eval = self.evaluate_baseline()
        
        if not winner:
            print("ERROR: No baseline winner found")
            return
            
        # Stage B: Cross-runs
        stage_b_results = self.run_stage_b(winner)
        
        # Final evaluation
        final_eval = self.final_evaluation()
        
        # Synthesize and prepare artifacts
        final_winner = self.synthesize_results(baseline_eval, final_eval)
        
        print("\n" + "="*80)
        print("EXPERIMENT COMPLETE")
        print(f"Final selected implementation: {final_winner}")
        print(f"Results saved to: {self.experiment_dir}")
        print("="*80)

if __name__ == '__main__':
    runner = ExperimentRunner()
    runner.run()